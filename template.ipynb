{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "loader = SimpleDirectoryReader('./data')\n",
    "documents = loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_kwargs = {'device': device}\n",
    "embed_model = HuggingFaceEmbeddings(model_name='BAAI/bge-base-en', model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.75s/it]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import CustomLLM, CompletionResponse, LLMMetadata, CompletionResponseGen\n",
    "from transformers import pipeline, BitsAndBytesConfig, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# set context window size\n",
    "context_window = 4096\n",
    "# set number of output tokens\n",
    "num_output = 150\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=device,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    model_kwargs={\"temperature\":0, \"quantization_config\": quantization_config},\n",
    ")\n",
    "\n",
    "class llama(CustomLLM):\n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        \"\"\"Get LLM metadata.\"\"\"\n",
    "        return LLMMetadata(\n",
    "            context_window=context_window,\n",
    "            num_output=num_output,\n",
    "            model_name=model_name\n",
    "        )\n",
    "\n",
    "    def complete(self, prompt: str, **kwargs) -> CompletionResponse:\n",
    "        prompt_length = len(prompt)\n",
    "        response = pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
    "        # only return newly generated tokens\n",
    "        text = response[prompt_length:]\n",
    "        return CompletionResponse(text=text)\n",
    "\n",
    "    def stream_complete(self, prompt: str, **kwargs) -> CompletionResponseGen:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "llm = llama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup service context\n",
    "from llama_index.core import Settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.num_output = num_output\n",
    "Settings.chunk_size = 512\n",
    "Settings.context_window = context_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " that you find interesting or unique.\n",
      "I find Poland to be a country with a rich history, culture, and traditions. One thing that I find particularly interesting is the country's complex and tumultuous past, which has had a significant impact on its present-day identity. Poland has been invaded and occupied by various powers throughout history, including the Mongols, the Swedes, the Prussians, and the Nazis. The country has also experienced significant periods of cultural and economic growth, such as the Renaissance and the Enlightenment.\n",
      "\n",
      "One unique aspect of Polish culture is the country's strong sense of national identity and its preservation of traditional customs and practices. For example, the country's folk architecture, such as the wooden houses and churches, is a testament to the\n"
     ]
    }
   ],
   "source": [
    "print(llama().complete(\"Tell me something about Poland\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "# dimensions of embed_model\n",
    "d = 768\n",
    "faiss_index = faiss.IndexFlatL2(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "\n",
    "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(documents=documents, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist(persist_dir=\"./index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import load_index_from_storage\n",
    "\n",
    "vector_store = FaissVectorStore.from_persist_dir(\"./index\")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store, persist_dir=\"./index\"\n",
    ")\n",
    "index = load_index_from_storage(storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 4cc82970-6823-4c37-8b4d-4102c951884e<br>**Similarity:** 0.4018879532814026<br>**Text:** What I Worked On\n",
       "\n",
       "February 2021\n",
       "\n",
       "Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\n",
       "\n",
       "The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data process...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** bd2b0d23-cc31-42a3-b8c2-e05727151ec2<br>**Similarity:** 0.4317491948604584<br>**Text:** I certainly did. So at the end of the summer Dan and I switched to working on this new dialect of Lisp, which I called Arc, in a house I bought in Cambridge.\n",
       "\n",
       "The following spring, lightning struck. I was invited to give a talk at a Lisp conference, so I gave one about how we'd used Lisp at Viaweb. Afterward I put a postscript file of this talk online, on paulgraham.com, which I'd created years before using Viaweb but had never used for anything. In one day it got 30,000 page views. What on e...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "\n",
    "retriever = index.as_retriever(similarity_top_k=2)\n",
    "retrieved_nodes = retriever.retrieve(\"What did the author do growing up?\")\n",
    "\n",
    "for node in retrieved_nodes:\n",
    "    display_source_node(node, source_length=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "custom_qa_template = (\n",
    "    \"<s>[INST] <<SYS>>\\n\"\n",
    "    \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible using the context text provided. Your answers should only answer the question once and not have any text after the answer is done.\\n\"\n",
    "    \"If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\n",
    "    \"<</SYS>>\\n\\n\"\n",
    "    \"CONTEXT: \\n\"\n",
    "    \"{context_str}\\n\\n\"\n",
    "    \"Question: \"\n",
    "    \"{query_str}\"\n",
    "    \"[/INST]\"\n",
    ")\n",
    "\n",
    "custom_refine_template = (\n",
    "    \"<s>[INST] <<SYS>>\\n\"\n",
    "    \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible using the context text provided. Your answers should only answer the question once and not have any text after the answer is done.\\n\"\n",
    "    \"If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\n",
    "    \"<</SYS>>\\n\\n\"\n",
    "    \"This is you previous answer:\\n\"\n",
    "    \"{existing_answer}\\n\\n\"\n",
    "    \"We have the opportunity to refine the existing answer (only if needed) with some more context below.\\n\\n\"\n",
    "    \"CONTEXT: \\n\"\n",
    "    \"{context_msg}\\n\\n\"\n",
    "    \"Question: \"\n",
    "    \"{query_str}\"\n",
    "    \"Do not put any metadata information in the answer as well as context only the answer itself.\"\n",
    "    \"[/INST]\"\n",
    ")\n",
    "\n",
    "custom_qa_template = PromptTemplate(custom_qa_template)\n",
    "custom_refine_template = PromptTemplate(custom_refine_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. He wrote short stories. 2. He worked on programming on the IBM 1401 computer. He wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep. The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district\\'s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain\\'s'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=2, prompt_template=custom_qa_template, refine_template=custom_refine_template)\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' There is no mention of \"epku\" in the provided text. The text appears to be an essay or a memoir written by Paul Graham, the founder of Y Combinator, about the early days of the startup accelerator. The text does not mention a person named \"epku\". If you have any further information or context about \"epku\", I can try to help you better.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=2, prompt_template=custom_qa_template, refine_template=custom_refine_template)\n",
    "response = query_engine.query(\"who is epku?\")\n",
    "response.response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
